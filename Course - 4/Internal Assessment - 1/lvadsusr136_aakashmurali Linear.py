# -*- coding: utf-8 -*-
"""Untitled7.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1R6b_eRpWWiUIw7cMR-dywbINGKH_ci97
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import random
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler , StandardScaler
from sklearn.metrics import accuracy_score, classification_report, precision_recall_fscore_support
from sklearn.metrics import precision_recall_curve,confusion_matrix, ConfusionMatrixDisplay
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score,mean_squared_error,mean_absolute_error

df=pd.read_csv('https://raw.githubusercontent.com/Deepsphere-AI/LVA-Batch5-Assessment/main/expenses.csv')
df

#exploration of data
df.shape
df.describe()

df.isnull().sum() #bmi has 16 null values , remaining columns doesnt have any
df['bmi'].fillna(df['bmi'].mean(),inplace=True) # we replace null values with mean of the column

df.duplicated().sum() # it has 1 duplicated data and is thus removed
df.drop_duplicates(inplace=True)

num_cols=df.select_dtypes(include=['int64','float64']).columns
print("numerical columns : " ,num_cols)

#boxplot is used to understand the outliers
for cols in num_cols:
  sns.boxplot(df[cols])
  plt.title(cols)
  plt.show()
  df[cols] = pd.to_numeric(df[cols], errors='coerce')
  q1 = df[cols].quantile(0.25)
  q3 = df[cols].quantile(0.75)
  iqr = q3 - q1
  lower_bound = q1 - 1.5 * iqr
  upper_bound = q3 + 1.5 * iqr
  clean_df = df[(df[cols] >= lower_bound) & (df[cols] <= upper_bound)]
print("DataFrame after removing outliers:")
print(clean_df)
df=clean_df

#find and handle the categorical features...

categorical_cols=df.select_dtypes(include='object').columns
print(categorical_cols)
encoded=pd.get_dummies(df,columns=categorical_cols)
encoded.head()

#reducing unwanted rows which can be same row with binary values , in this case male = 1 , female =0 , no=0 , yes =1

encoded.drop(['sex_female','sex_male'],axis=1,inplace=True)
encoded.drop(['smoker_no','smoker_yes'],axis=1,inplace=True)
encoded['smoker']=df['smoker']
encoded['sex']=df['sex']
encoded['sex']=encoded['sex'].map({'male':1,'female':0})
encoded['smoker']=encoded['smoker'].map({'yes':1,'no':0})
cols=encoded.select_dtypes(include='boolean').columns
encoded[cols]=encoded[cols].astype(int)
encoded[cols]=MinMaxScaler().fit_transform(encoded[cols])
encoded.head()

print(df.columns)
# age,sex,region,smoker,bmi,children... everything is needed to determine the charge therfore it is decided not to remove any features
#duplicates , null values , outliers , all of these are already handled

x=encoded[['age',	'bmi'	,'children',	'charges'	,'region_northeast'	,'region_northwest',	'region_southeast'	,'region_southwest',	'smoker',	'sex']]
y=encoded['charges']
x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.3)
model=LinearRegression()
model.fit(x_train,y_train)
pred = model.predict(x_test)

r2 = r2_score(y_test, pred)
mse = mean_squared_error(y_test, pred)
mae = mean_absolute_error(y_test, pred)
print("r-square:",r2,"\n\nmse:",mse,"\n\nmae:",mae)

plt.scatter(y_test, pred, c='red', label='Actual')
plt.scatter(y_test, y_test, c='blue', label='Predicted')
plt.xlabel("Actual values")
plt.ylabel("Predicted values")
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='black', linestyle='-', label='bestfitline')
plt.legend()