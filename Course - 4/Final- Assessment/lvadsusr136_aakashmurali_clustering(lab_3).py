# -*- coding: utf-8 -*-
"""lvadsusr136_aakashmurali_clustering(lab 3).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gz4XX658KriAXz64aJTjullxS4RcxHPW
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import time
from sklearn.metrics import accuracy_score, classification_report, precision_recall_fscore_support
from sklearn.metrics import precision_recall_curve,confusion_matrix, ConfusionMatrixDisplay
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score,mean_squared_error,mean_absolute_error
from imblearn.over_sampling import SMOTE
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.cluster import KMeans
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier , RandomForestRegressor
from sklearn.ensemble import IsolationForest
import collections
from sklearn.model_selection import train_test_split
from imblearn.over_sampling import SMOTE
from imblearn.metrics import classification_report_imbalanced
from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, accuracy_score, classification_report,silhouette_score
from sklearn.model_selection import KFold, StratifiedKFold
import random
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler , LabelEncoder
from sklearn.metrics import accuracy_score, classification_report, precision_recall_fscore_support, precision_recall_curve
import warnings
warnings.filterwarnings("ignore")

df=pd.read_csv('https://raw.githubusercontent.com/Deepsphere-AI/LVA-Batch5-Assessment/main/customer_segmentation.csv')
df.head()

#understanding the data
df.shape
df.info()

#checking for null values
df.isnull().sum()

#replacing the null values in the income column with its mean value
df['Income']=df['Income'].fillna(df['Income'].mean())

#no need to check for outliers in the clustering model
#understanding the duplicates
df.duplicated().sum()

#encoding the categorical rows
encode=LabelEncoder()
for col in df.select_dtypes(include='object').columns:
  df[col]=encode.fit_transform(df[col])
df.head()

#understanding the co-realtion with a heatmap
corr=df.corr()
plt.figure(figsize=(25,20))
sns.heatmap(corr,annot=True,fmt='.2f')
plt.show()

#having the least co-relation , some of the features are removed
df.drop(['ID','Marital_Status','Year_Birth'],axis=1,inplace=True)

df.head()

#we use elbow-method to find the optimal k-value
sse=[]
for i in range(1,11):
  km=KMeans(n_clusters=i)
  km.fit(df)
  sse.append(km.inertia_)

plt.plot(range(1,11),sse)
plt.show()

km=KMeans(n_clusters=3)
km.fit(df)
pred=km.predict(df)

print("Silhouette score : ",silhouette_score(df,pred))

#plotting , with 3 clusters , 1 cluster being at the right bottom
plt.scatter(df['Income'],df['MntMeatProducts'],c=pred)
plt.show()